
from embeddings_reader import EmbeddingsReader
from tokenizer import Tokenizer
#from low_level_encoder import LowLevelEncoderStub
from low_level_encoder import LowLevelEncoder
from mlm_learning_net import Transformer_tr

from mlm_shared import CATEG_DIMS, ASSOC_DIMS, GRAMM_DIMS
from mlm_shared import DECO_LAYERS, DECO_DIMS, DECO_HEADS, DECO_FF


import sentencepiece as spm
SP_BOS_ID = 1 # идентификатор begin_of_sequence в sentencepiece-модели
#SP_PAD_ID = 3 # идентификатор pad в sentencepiece-модели

import tensorflow as tf
import numpy as np


# ENCODER_FIXED_SEQ_LEN = 25
# DECODER_MAX_SEQ_LEN = 8

# my_sent_list = [
#                     [["прошлую", "пятницу", "в", "кремлевском", "дворце", "проходило", "заседание", "Совбеза", "России", "по", "вопросам", "продовольственной", "безопасности", ".", "На", "встрече", "В", ".", "В", ".", "Путин", "и", "Д", ".", "А"], ["."]]
#                   , [["этого", "Васька", "побежал", "домой", ".", "Рассказывая", "матери", "о", "случившемся", ",", "он", "все", "время", "посматривал", "на", "часы", ",", "чтобы", "не", "пропустить", "момент", ",", "когда", "должны", "были"], ["подойти"]]
#                   , [["в","магазине","были","выбиты","стекла","витрины","и","вскрыта","касса",".","Преступника","задержали","по","горячим","следам",".","Первое","судебное","заседание","прошло","летом",".","А","в","декабре"],["2012"]]
#                   , [["Новости",".","Резонансное","ДТП","с","участием","автомобиля","Toyota","произшошло","вчера","на","улице","Строителей",".","Вошедшая","в","занос","иномарка","сбила","60-летнего","мужчину",",","переходившего","дорогу","по"],["пешеходному"]]
# #                  , [["","","","","","","","","","","","","","","","","","","","","","","","",""],[""]]
#                ]


# ENCODER_FIXED_SEQ_LEN = 14
# DECODER_MAX_SEQ_LEN = 10

# my_sent_list = [
#                     [["продовольственной", "безопасности", ".", "На", "встрече", "В", ".", "В", ".", "Путин", "и", "Д", ".", "А"], ["."]]
#                   , [["все", "время", "посматривал", "на", "часы", ",", "чтобы", "не", "пропустить", "момент", ",", "когда", "должны", "были"], ["подойти"]]
#                   , [["задержали","по","горячим","следам",".","Первое","судебное","заседание","прошло","летом",".","А","в","декабре"],["2012"]]
#                   , [["улице","Строителей",".","Вошедшая","в","занос","иномарка","сбила","60-летнего","мужчину",",","переходившего","дорогу","по"],["пешеходному"]]
#                   , [["Строителей",".","Вошедшая","в","занос","иномарка","сбила","60-летнего","мужчину",",","переходившего","дорогу","по", "пешеходному"],["переходу"]]
#                   , [["Вошедшая","в","занос","Toyota", "Land", "Cruiser","сбила","60-летнего","мужчину",",","переходившего","дорогу","по", "пешеходному"],["переходу"]]
#                   , [["Вошедшая","в","занос","Toyota", "Land", "Cruiser","сбила","пожилую","женщину",",","переходившую","дорогу","по", "пешеходному"],["переходу"]]
#                   , [["Проехавший","на","красный","свет","автомобиль","Рено","сбил","пожилую","женщину",",","переходившую","дорогу","по", "пешеходному"],["переходу"]]
# #                  , [["","","","","","","","","","","","","",""],[""]]
#                ]

ENCODER_FIXED_SEQ_LEN = 12
DECODER_MAX_SEQ_LEN = 5

my_sent_list = [
                    [[".", "На", "встрече", "В", ".", "В", ".", "Путин", "и", "Д", ".", "А"], ["."]]
                  , [["посматривал", "на", "часы", ",", "чтобы", "не", "пропустить", "момент", ",", "когда", "должны", "были"], ["подойти"]]
                  , [["горячим","следам",".","Первое","судебное","заседание","прошло","летом",".","А","в","декабре"],["2012"]]
                  , [[".","Вошедшая","в","занос","иномарка","сбила","60-летнего","мужчину",",","переходившего","дорогу","по"],["пешеходному"]]
                  , [["Вошедшая","в","занос","иномарка","сбила","60-летнего","мужчину",",","переходившего","дорогу","по", "пешеходному"],["переходу"]]
                  , [["занос","Toyota", "Land", "Cruiser","сбила","60-летнего","мужчину",",","переходившего","дорогу","по", "пешеходному"],["переходу"]]
                  , [["занос","Toyota", "Land", "Cruiser","сбила","пожилую","женщину",",","переходившую","дорогу","по", "пешеходному"],["переходу"]]
                  , [["красный","свет","автомобиль","Рено","сбил","пожилую","женщину",",","переходившую","дорогу","по", "пешеходному"],["переходу"]]
                  , [[".","Все","расселись","по","своим","местам",",","а","хитро","улыбавшийся","Ромка","подмигнул"],["Машке"]]
                  , [["Капитан","сборной","Алексеев","сегодня","проводит","четырнадцатый","матч","на","турнире","и","имеет","в"],["зачете"]]
                  , [["Российские","предприятия","ВПК","начали","подготовку","к","серийному","производству","этих","танков",",","а"],["также"]]
                  , [["Представители","китайского","бизнес-сообщества","склонны","полагать",",","что","финансовый","кризис",",","начавшийся","в"],["США"]]
                  , [["Китайский","автопром","порадовал","посетителей","выставки","новинками","от","компании","BYD","и","машинами","с"],["электрическим"]]
                  , [["Выступление","хора","Триумф","будет","проходить","в","городском","концертном","зале","в","пятницу","около"],[""]]
                  , [["Органы","внутренних","дел","Казахстана","оценили","величину","ущерба","от","преступных","действий",",","совершенных"],["мошенниками"]]
                  , [["Нефтетрейдеры","зафиксировали","колоссальную","прибыль","после","того",",","как","стало","известно",",","что"],[""]]
                  , [["Помытые","баклажаны","нужно","порезать","кружочками","и","обмакнуть","в","тесто",",","а","томаты"],["посолить"]]
                  , [["Убегая","от","преследователей",",","Маша","выронила","ключи","от","дома","и","теперь","хотела"],["вернуться"]]
                  , [["На","столе","у","отца","лежала","шкатулка",",","несколько","карандашей","и","пожелтевшие","от"],["старости"]]
                  , [["Д",".","А",".","Медведев","в","среду","провел","совещание","с","представителями","думских"],["фракций"]]
                  , [["Цезарь","продолжал","поход","в","галльские","земли","и","уже","почти","добрался","до","самых"],["отдаленных"]]
#                  , [["","","","","","","","","","","",""],[""]]
               ]


# ENCODER_FIXED_SEQ_LEN = 8
# DECODER_MAX_SEQ_LEN = 4

# my_sent_list = [
#                     [["прошлую", "пятницу", "в", "кремлевском", "дворце", "проходило", "заседание", "Совбеза", ], ["России"]]
#                   , [["чтобы", "не", "пропустить", "момент", ",", "когда", "должны", "были"], ["подойти"]]
#                   , [["судебное","заседание","прошло","летом",".","А","в","декабре"],["2012"]]
#                   , [["иномарка","сбила","60-летнего","мужчину",",","переходившего","дорогу","по"],["пешеходному"]]
#                   , [["Салавата","Юлаева","\"","в","матче","регулярного","чемпионата","Континентальной"],["хоккейной"]]
#                   , [["приобретен","без","ввода","дополнительных","данных",",","зрителю","не"],["придется"]]
#                   , [["а","рядом","размещали","все",",","что","может","понадобиться"],["в"]]
#                   , [["ими","почка","генетически","модифицированной","свиньи","выжила","в","теле"],["макаки"]]
#                   , [["Сообщения",",","которые","распространяются","на","страницах","и","платформах"],["вооруженных"]]
#                   , [["в","этих","городах","укрепленную","оборонительную","линию","и","полностью"],["готовы"]]
# #                  , [["","","","","","","",""],[""]]
#                ]



# Загрузим векторную модель (одновременно добавив в нее служебные векторы)
print('Loading embeddings...')
vm = EmbeddingsReader("vectors-rue.c2v")
# Создадим токенизатор
print('Creating tokenizer...')
tokenizer = Tokenizer(vm)
# Загрузим sentencepiece-модель (чтобы узнать размер словаря)
print('Loading SentencePiece model...')
sp = spm.SentencePieceProcessor(model_file='sp_m.model')
target_vocab_size = sp.get_piece_size()

encoded_msl = []
for s, tail in my_sent_list:
    se = tokenizer.tokens2subtokens([w.lower() for w in s])
    taile = [] # (без подсказки)
    #taile = [sp.encode(tail, out_type=int)[0][0]] # (подсказка одного токена первого слова)
    #taile = sp.encode(tail, out_type=int)[0] # (подсказка всего первого слова)
    taile = [SP_BOS_ID] + taile
    encoded_msl.append([se,taile])
#     print(se)
#     ssp = sp.encode(' '.join(s).lower(), out_type=int)
#     ssp = [1] + ssp + [2]
#     print(ssp)

# создаем модель, загружаем веса
print('Creating model...')
lle_layer = LowLevelEncoder( CATEG_DIMS, ASSOC_DIMS, GRAMM_DIMS, 
                                  vm.stems_count, vm.sfx_count, 
                                  ctxer_training=False, balance_training_cat=False, balance_training_ass=False, balance_training_gra=False, return_attention=False,
                                  name='lle' )
model = Transformer_tr( lle_layer, DECO_LAYERS, DECO_DIMS, DECO_HEADS, DECO_FF, target_vocab_size, 
                        encoder_seq_len=ENCODER_FIXED_SEQ_LEN, decoder_seq_len=DECODER_MAX_SEQ_LEN, train_transformer=False, trainable=False, name='EDM' )
model.build(input_shape = [(None, ENCODER_FIXED_SEQ_LEN, 2), (None, DECODER_MAX_SEQ_LEN)])
# data_stub_i = tf.random.uniform((2, MAX_LEN, 2), dtype=tf.int32, minval=2, maxval=10) 
# data_stub_t = tf.random.uniform((2, 2*MAX_LEN), dtype=tf.int32, minval=2, maxval=10)
# model([data_stub_i, data_stub_t]) 
model.load_dec_weights('dec_model')
lle_layer.load_embeddings(vm)
lle_layer.load_own_weights('lle_weights')
model.summary()


# выполняем генерацию (предсказание)
print('Predict...')
for src, enc_src in zip(my_sent_list, encoded_msl):
    print(src)
    inp = tf.convert_to_tensor(enc_src[0], dtype=tf.int32)
    inp = tf.expand_dims(inp, 0)
    out = enc_src[1]  # init output
    for i in tf.range(DECODER_MAX_SEQ_LEN-len(enc_src[1])):
        tar_i = tf.constant(out)
        tar_i = tf.expand_dims(tar_i, 0)
#        print(inp)
#        print(tar_i)
        pred = model([inp, tar_i], training=False)
#        print(pred)
        pred = pred[:, -1:, :] # select the last token from the seq_len dimension -- (batch_size, 1, vocab_size)
        pred_id = tf.argmax(pred, axis=-1)
        out.append(int(pred_id[0]))  # снимаем batch и превращаем в скаляр
#         break
    print(sp.decode(out)) # выводим, начиная со слова подсказки

        

    print()






